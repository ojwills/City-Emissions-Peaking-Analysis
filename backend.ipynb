#!/Users/oliverwills/anaconda3/bin/python3
#Import data science libraries
import pandas as pd
import numpy as np
#Caclulate current year for use in peaking criteria 
from datetime import date
current_year = date.today().year
#Set display options
pd.options.display.float_format = '{:.0f}'.format
pd.set_option('display.max_columns', 10000)
pd.set_option('display.max_rows',1000)

def read_in_data_from_2017_peaking_analysis(path):
    """
    Reads in data from 2017 peaking analysis and reshapes for use in programme.
    INPUT: Excel file (.xlsx)
    OUTPUT: Pandas DataFrame
    """
    df = pd.read_excel(path, sheet_name = 'Mastersheet', header = 2)
    df = df.iloc[0:80,0:50] #select relevant rows and columns 
    df.drop('Protocol', axis = 1, inplace = True) #drop protocol column 
    #df = df[df['Peaked? (Criteria 1, 2, 3 and 4 all met)']!='Do not use'] #filter out records marked as "do not use"
    df = df.iloc[0:80,0:31] #select relevant columns (ignore 'peaked criteria and helper columns')
    df = pd.melt(df, id_vars=["City", "Data Source"], var_name="Year", value_name="Emissions").sort_values(['City', 'Year']).reset_index(drop=True) #reshape dataframe
    df['Data Source'] = df['Data Source'].apply(clean_data_source) #clean data source columns [TEMPORARY FIX]
    df.dropna(subset=['Emissions'], inplace = True) #Drop rows where emissions is zero 
    return df

def clean_data_source(x):
    """
    Temporary fix while data sources are not updated in peaking analysis
    """
    if x == 'A':
        return 1
    if x == 'B':
        return 2
    if x == 'GHG' or x == 'CO2 only':
        return 7
    if pd.isnull(x):
        return 2
    if x == 2:
        return 1 
    else:
        return x

def read_in_data_from_gpc_tracker(path):
    """
    Reads in data from GPC Tracker and reformats for use in programme.
    INPUT: Excel file (.xlsx)
    OUTPUT: Pandas DataFrame
    """
    df = pd.read_excel(path, sheet_name = 'GPC data_Live', header = 2) #Read in gpc tracker data
    df = df[['Use for GPC Dashboard?', 'City', 'Year_calendar', 'BASIC']] #Select relevant columns 
    df = df[df['Use for GPC Dashboard?'] == 'Yes'].reset_index(drop=True) #Filter rows marked for use in GPC Dashboard
    df.rename(columns={'Year_calendar': 'Year', 'BASIC':'Emissions'}, inplace=True) #Rename to align with peaking analysis
    df['Data Source'] = 1 #Add data source column and code all data points as data source 1
    df.dropna(subset=['Emissions'], inplace = True) #Drop rows where emissions is zero 
    df = df[['City','Data Source','Year','Emissions']].sort_values(['City','Year']) #Reorder columns and sort
    return df

def join_data_from_peaking_analysis_and_gpc_tracker(df1, df2):
    """
    Combines data from 2017 Peaking Analysis and GPC Tracker into a single DataFrame and drops duplicate rows
    INPUT: 2 DataFrame
    OUTPUT: 1 DataFrame
    """
    df = df1.append(df2).drop_duplicates().sort_values(['City', 'Data Source', 'Year']).reset_index(drop=True) 
    df = df[df['City']!='Basel'] #Remove Basel from the DataFrame as it is no longer a C40 city
    return df

def calculate_peak_emissions(df, current_year):
    """
    Analyses city GHG emissions to determine if they have peaked
    INPUT: DataFrame containing peaking analysis and GPC Tracker GHG emissions
    OUTPUT: DataFrame with assessment of whether each city has peaked
    """
    
    def reshape_data(df):
        """
        Reshapes DataFrame so that each city and datasource has a unique row with years as a column header
        INPUT: DataFrame
        OUTPUT: Reshaped DataFrame
        """
        df = df.pivot_table(values='Emissions', index=['City','Data Source'], columns='Year', aggfunc='first').reset_index().fillna(0)
        df.columns.name = None #set columns index name to none 
        return df

    def calculate_peaking_parameters(df):
        """
        Caclulatea parameters used to assess whether city has peaked emissions
        INPUT: DataFrame
        OUTPUT: DataFrame with peakign parameters as 4 additional columns
        """
        cols = df.columns.difference(['City','Data Source'])
        df['Num data points'] = df[cols].gt(0).sum(axis=1)
        df['Max emissions'] = df[cols].max(axis =1)
        df['Max emissions year'] = df[cols].idxmax(axis =1)
        df['Recent emissions'] = df[cols].apply(lambda x: x.iloc[x.nonzero()].iloc[-1], axis=1)
        df['Recent emissions year'] = df[cols].apply(lambda x: x.iloc[x.nonzero()].index[-1], axis=1)
        return df
    
    def apply_peaking_criteria(df, current_year):
        """
        Calculates pearking criteria using peaking parameters
        INPUT: DataFrame
        OUTPUT: DataFrame with Boolean assessment for each peaking criteria
        """
        #Peaking Criteria 1: At least 3 years of data available?
        df['PC1'] = (current_year - df['Recent emissions year'] >= 3)
        #Peaking Criteria 2: Max emissions >5 years before recent inventory?
        df['PC2'] = (df['Recent emissions year'] - df['Max emissions year'] >= 5)
        #Peaking Criteria 3: Recent inventory < 5 years old
        df['PC3'] = (current_year - df['Recent emissions year'] <= 5)
        #Peaking Criteria 4: Max emissions >10% higher than recent inventory
        df['PC4'] = ((df['Max emissions']-df['Recent emissions'])/df['Recent emissions']) >= 0.1
        return df
    
    def calculate_peak_emissions_status(df):
        """
        Analyses peaking criteria to assess whether city has peaked
        INPUT: DataFrame
        OUTPUT: DataFrame with peaking assessment as additional columne
        """
        
        def f(x):
            if x['PC1'] & x['PC2'] & x['PC3'] & x['PC4']: #If all peaking criteria are TRUE returns 'PEAKED'
                return 'PEAKED'
            elif x['PC1'] & (not x['PC2']) & x['PC3'] & (not x['PC4']): #If first 3 peaking critiera are TRUE returns 'NOT PEAKED'
                return 'NOT PEAKED'
            else: #Otherwise returns 'CANNOT TELL'
                return 'CANNOT TELL'
        
        df['Peak Status'] = df.apply(f, axis = 1) #Calculates columns by applying above function across rows 
        return df
        
    def rename_columns(df):
        """
        Renames columns for ease of understanding
        """
        df.rename(columns ={
            'PC1':'PC1: At least 3 year of years of data available',
            'PC2':'PC2: Max emissions >5 years before recent inventory',
            'PC3':'PC3: Recent inventory <5 years old',
            'PC4':'PC4: Max emissions <10% higher than recent inventory'},
            inplace = True)
        return df 
  
    df = reshape_data(df)
    df = calculate_peaking_parameters(df)
    df = apply_peaking_criteria(df, current_year)
    df = calculate_peak_emissions_status(df)
    df = rename_columns(df)
    return df

def select_duplicate_cities_to_use_in_dashboard(df):
    """
    Sorts input dataframe by city, peak status, number data points and data source and removes duplicates keeping the
    first record. Duplicate records are retained in this order: Status = PEAKED > Status = NOT PEAKED > Highest number
    data points > lowest numerical data source code. 

    INPUT: DataFrame 
    
    OUTPUT: DataFrame with sorted cities conaining NO duplicates for use in peaking analysis dashboard 

    """
    df = df.sort_values(['City', 'Peak Status', 'Num data points', 'Data Source'], ascending = [True, False, False, True])
    
    #Initialise values for loop 
    ref_city = df['City'][0]
    df.loc[0, 'Use for dashboard?'] = 'Y'
    
    for index, row in df.iterrows(): #iterate through rows in DataFrame
        city = row['City']
        if city != ref_city:
            df.loc[index,'Use for dashboard?'] = 'Y'
            ref_city = city
        else:
            df.loc[index, 'Use for dashboard?'] = 'N'
    return df

def reshape_data_for_dashboard(df):
    """
    Reshapes data for use in the Qlik dashboatd
    INPUT: DataFrame (Years as rows)
    OUTPUT: DataFrame (Years as column)
    """
    df = df[df['Use for dashboard?'] == 'Y'] #filter rows to be used in dashboard 
    cols = df.columns.difference(['Num data points', 'Max emissions', 'Recent emission', 'Recent emissions year', 'PC1: At least 3 year of years of data available', 'PC2: Max emissions >5 years before recent inventory', 'PC3: Recent inventory <5 years old', 'PC4: Max emissions <10% higher than recent inventory', 'Use for dashboard?'])
    df = df[cols] #Select columns for use in dashboard
    df = pd.melt(df, id_vars=["City", "Data Source", "Peak Status", "Max emissions year"], var_name="Year", value_name="Emissions").sort_values(['City', 'Year']).reset_index(drop=True)
    df['Peak year'] = df.apply(lambda x : 1 if x['Max emissions year'] == x['Year'] and x['Peak Status'] == 'PEAKED' else 0, axis=1)
    df.drop('Max emissions year', axis = 1, inplace = True)
    df = df[['City', 'Data Source', 'Year', 'Emissions', 'Peak Status', 'Peak year']]
    return df

def generate_dataframes(path_1, path_2):
    """
    Generates DataFrames used in the programme by calling above functions
    INPUT: File paths to 2017 Peaking Analysis and GPC Tracker
    OUTPUT: Tuple of 6 DataFrames
    """
    df1 = read_in_data_from_2017_peaking_analysis(path_1)
    df2 = read_in_data_from_gpc_tracker(path_2)
    df3 = join_data_from_peaking_analysis_and_gpc_tracker(df1, df2)
    df4 = calculate_peak_emissions(df3,current_year)
    df5 = select_duplicate_cities_to_use_in_dashboard(df4)
    df6 = reshape_data_for_dashboard(df5)
    return (df1, df2, df3, df4, df5, df6)

def write_to_excel(results):
    """
    Writes dataframes to Excel
    INPUT: Tuple of 6 DataFrames
    OUTPUT: None
    """
    #Create a Pandas Excel writer using XlsxWriter as the engine
    writer = pd.ExcelWriter('/Users/oliverwills/desktop/peaking_emissions_dashboard.xlsx', engine='xlsxwriter')
    #Write each dataframe to a different worksheet
    results[4].to_excel(writer, sheet_name='MASTER_Peak_Emissions', index = False)
    results[5].to_excel(writer, sheet_name='DASHBOARD_Peak_Emissions', index = False)
    #Close the Pandas Excel writer and output the Excel file
    writer.save()

def run_programme(path_1, path_2):
    results = generate_dataframes(path_1, path_2)
    write_to_excel(results)
    return results

path_1 = '/Users/oliverwills/Box/C40 (internal)/Regions and Cities (internal)/M&P/04_Analytics/03_Other analytics/01_Peaking analysis/00_GCAS/00 Master_Peaking analysis data review_20180905 NR.xlsx'
path_2 = '/Users/oliverwills/Box/C40 (internal)/Regions and Cities (internal)/M&P/04_Analytics/00_Raw data/01_Emissions/Live tracker/01_GPC Inventory Tracker.xlsx'

run_programme(path_1,path_2)
